# FrostKiwi's treasure chest
This is a collection of useful things I want to share with the world.
 - [Genshin Impact Anki deck](https://github.com/FrostKiwi/treasurechest#genshin-impact-anki-deck)
   - [Why Genshin Impact?](https://github.com/FrostKiwi/treasurechest#why-genshin-impact)
   - [Info and Structure](https://github.com/FrostKiwi/treasurechest#info-and-structure)
   - [How it was captured](https://github.com/FrostKiwi/treasurechest#how-it-was-captured)
 - [Jōyō kanji Unicodes lists](https://github.com/FrostKiwi/treasurechest#j%C5%8Dy%C5%8D-kanji-unicodes-lists)
   - [Regional variants](https://github.com/FrostKiwi/treasurechest#regional-variants)
 - [Jo_MPEG converted to C](https://github.com/FrostKiwi/treasurechest#jo_mpeg-converted-to-c)
 - [Just-a-textbox](https://github.com/FrostKiwi/treasurechest#just-a-textbox)
 - [WaniKani Japanese prompts - userscript](https://github.com/FrostKiwi/treasurechest#wanikani-japanese-prompts---userscript)
## Genshin Impact Anki deck
It's quite the tradition among Japanese learners to publish parts of their Anki [Mining](https://animecards.site/yomichansetup/#setting-up-yomichan) decks, so others may get inspired by them or straight up use them. This ~1000 note deck is an excerpt of my Mining deck, which was/is being created in-part from the video game [Genshin Impact](https://genshin.hoyoverse.com/en/home). This post will go into the thought process behind the deck, how it was created and has sound clips in this GitHub page below every screenshot for reference (muted by default on Github, gotta unmute before playing). Of course, using someone else's Mining deck doesn't carry nearly the same benefit as making one yourself, so this is mainly to just document my workflow. [**Link to the deck on Ankiweb**](https://ankiweb.net/shared/info/870567459) (If AnkiWeb ends up pulling the deck due to copyright concerns, a copy is [here](https://drive.google.com/file/d/1DmO1B0cH0d9wQ-YAkrJ9t9pJR3DCI9Gq/view?usp=sharing))

All cards have in-game sound + screenshot and almost all have additionally a dictionary sound file + pitch accent.
![image](https://user-images.githubusercontent.com/60887273/167257089-8ebc1518-8e91-4b0d-b19b-e23da4e2fb74.png)

https://user-images.githubusercontent.com/60887273/167257204-32647df3-6c05-4828-bd0a-3fffbb9a3e7d.mov

https://user-images.githubusercontent.com/60887273/167257205-b48096e7-be3e-459b-a358-e302c45f6bf6.mov
### Why Genshin Impact?
A couple of things come together to make Genshin quite the enjoyable learning experience. The obvious first: Except minor side quests, all dialogs are voiced and progress sentence by sentence, click by click, as is common in JRPGs and visual novels. This gives enough time to grasp the dialog's content. Funnily enough, in-game time does not stop during dialogs except in some quests, so sometimes multiple in-game days would pass by, as I grasped the contents of a dialog.

Another point is the writing style. A hotly debated topic in the player-base, is whether or not the addition of Paimon hurts the delivery of the story. The character constantly summarizes events happening and repeats commands or requests, that were given by another character just moments ago during dialog. The main criticism often brought up is that this makes the story-flow very child-like, which is a rather obvious design goal of the game - catering to a younger audience. What may be a sore in the eyes of many a player though, is a godsend in the eyes of a language learner.
Paimon often describes a situation, which was witnessed by the player mere moments ago, making the actual statement of a sentence trivial to understand.
![image](https://user-images.githubusercontent.com/60887273/167426600-cea0b78e-a6a4-4a01-b88a-8e0392325046.png)

https://user-images.githubusercontent.com/60887273/167430485-cb3aee2c-f7cc-4720-bb98-da153c34b39e.mov

https://user-images.githubusercontent.com/60887273/167430493-4b8be3cd-5d7b-4fb9-b5aa-f854660d7510.mov

And sometimes Paimon straight up becomes a dictionary herself and defines a word like a glossary text would.
![image](https://user-images.githubusercontent.com/60887273/167430735-7d2e709e-df54-488a-a9c5-ee27df7d03ed.png)

https://user-images.githubusercontent.com/60887273/167431180-673f5fad-4364-4d05-82cf-52a035669cfd.mov

https://user-images.githubusercontent.com/60887273/167431189-61af2101-fc47-45ef-8731-ee1ac8d23289.mov

There is a very dialog- and lore-heavy story-line in Dragonspine involving Albedo, which has some of the most information dense dialog of the game. Paimon often commented, how she completely lost the plot and didn't understand anything. During that story line I felt as if Paimon was sympathizing with me, as I battled my way to understanding that story-line. For me, Paimon really made this game shine as a learning tool.
#### The good outcome
I'm constantly surprised how much Genshin has propelled my speech forward. Similar to a movie you can quote decades later or video game moments being etched into memory, there is something about the media we consume, that makes it stick. I found myself using and recalling vocabulary acquired via Genshin faster than from other sources. Or maybe it's just that video games make you engage with their world and characters for far longer and with much more intensity, than other forms of indirect-study could.
#### The dangerous outcome
It's common knowledge that media uses artistic delivery in speech, has speech patterns rarely used in everyday life and uses a stylized way of writing. Basically all of it is [役割語](https://ja.wikipedia.org/wiki/%E5%BD%B9%E5%89%B2%E8%AA%9E). And yet, knowing that I still managed to trip up in minor ways. Case in point:
![image](https://user-images.githubusercontent.com/60887273/167408083-1cce9a93-172b-4e2a-b7b7-6f8937bbce63.png)

https://user-images.githubusercontent.com/60887273/167408470-6a448385-93c1-4f10-bbd5-a08939e577cd.mov

https://user-images.githubusercontent.com/60887273/167408479-d7b7847b-7f40-4577-a74f-0e98a936a6c1.mov

I used the 誉れです expression instinctively from time to time and just recently someone noted, that this expression has a rather archaic, regal tone. It was quite the funny situation, but it goes to show, that even knowing what kind of media I was consuming still didn't save me from tripping up. Coming from outside the language it's unavoidable to misinterpret an expression's nuance I guess. Though in this case, the in-game dialog should have really tipped me off, as the character speaking, Ninguang, uses it to tell a joke with a somewhat sarcastic undertone.
### Info and Structure
 - Since this is a mining vocabulary deck, it carries words \*I\* did not know during in-game dialog. I already finished the [Improved Core3k](https://ankiweb.net/shared/info/1060896809) deck, so there are zero duplicates between this deck and Core3k. Besides that, I started the deck shortly before my N4 exam and am now N3. Words in the deck are essentially N3 and up, with some easy ones sprinkled in. No in-universe words are saved, like モラ or 目狩り令.
 - The deck captures the main story-line and a few side-quests / story-quests from the beginning up to Inzuma's second chapter.
 - I always learn both Japanese -> English, as well as English -> Japanese. This point is hotly debated, whether or not it's useful or a massive waste of time. For me switching to learning both directions has been nothing but great, but it is not the default on Anki Web and not a popular opinion it seems. (If I can name a synonym in the English -> Japanese direction, I still let it pass as HARD and as AGAIN if I can only name a synonym, once the card returns) To fit with the default, I have disabled the English -> Japanese Card type.
 - For the in-game subtitles OCR sometimes failed. I corrected small mistakes, but when it output complete garbage, I added a cropped version of just the text in the screenshot for the sentence section. I did not always double check the OCR output, so mistakes will come up in the in-game dialog's transcript occasionally. If something looks weird with the example sentences, check the in-game screenshot for the correct transcript. 
 - To fit inside the AnkiWeb limit of 256 MB, all images were resized from the 1080p originals to fit a 1366x768 rectangle with aggressive 81% JPEG Quality and in-game dialog are mono MP3 files.
 - When I write "here: ..." I am referring to a word being used in a more specialized sense in the in-game dialog, like 人目 vs 人目を忍ぶ. In those cases two definitions are provided. This is to make the learning process a bit more compact and to prevent not being able to translate a sentence, whilst having just half of the definition.

![hitome](https://user-images.githubusercontent.com/60887273/167301152-40a0d15a-20f1-40ab-93ab-b7d86a6e591e.png)

https://user-images.githubusercontent.com/60887273/167292140-f3489c91-71b7-40d9-aedc-c630adc47da9.mov

https://user-images.githubusercontent.com/60887273/167292143-10c8668d-da68-404a-9b93-aeec9fabedc8.mov
- Characters speaking Kansai dialect have received there own tag "Kansaidialect".

![meccha](https://user-images.githubusercontent.com/60887273/167292541-94b7a289-e5fd-4a07-a80a-6b3f70381af1.png)

https://user-images.githubusercontent.com/60887273/167292350-45546385-7418-4d26-867e-cc020047a027.mov
#### Grammar
I also have a bunch of grammar cards mixed in, when I encountered new pieces of grammar and recognized it as such. For those I pasted the excellent [JLPT Sensei](https://jlptsensei.com/) summary images.

![kireicake](https://user-images.githubusercontent.com/60887273/167289347-d798a637-fd49-46c7-a2fd-2a2170cce7ec.png)

https://user-images.githubusercontent.com/60887273/167289579-61ae0167-825f-45a9-8161-795d8597073c.mov

https://user-images.githubusercontent.com/60887273/167289574-6af26096-7501-45bb-a5ae-8d6cd16d3eed.mov

A big surprise to me was the [YomiChan](https://github.com/FooSoft/yomichan) dictionary ["KireiCake"](https://foosoft.net/projects/yomichan/#dictionaries) having URL-shortened links from time to time, like [waa.ai/v4YY](https://waa.ai/v4YY) in the above card. In this case it leads to an [in-depth discussion on Yahoo](https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q1317655948) about that grammar point. [(Archive Link, in case it goes down)](http://web.archive.org/web/20220508092155/https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q1317655948) The love and patience of the Japanese learning online community is truly magnificent. From /djt/ threads on image-boards to [user-scripts connecting Kanji learn services to a collection of example recordings from Anime.](https://community.wanikani.com/t/userscript-anime-context-sentences/54003?u=frostkiwi) Stuff like that has me in awe.
#### To translate or not to translate
In the beginning I did toggle to English to screenshot the English version for the card's back-side, see the example card below. However, on recommendation from members of the [English-Japanese Language Exchange discord server](https://discord.com/invite/japanese), I stopped doing so. Mainly, because of localization discrepancies between both versions. Differences got especially heavy, when more stylized dialogue got involved. But also in part, because this is not recommended for mining in general. Quoting from the Core3k description:
> Don't use the field 'Sentence-English' in your mined cards. In fact, get rid of it once you have a solid understanding of Japanese. When you mine something you should already have understood the sentence using the additional information on your cards.

![image](https://user-images.githubusercontent.com/60887273/167288657-68cde527-88a7-480c-9aef-3f78191781b4.png)

https://user-images.githubusercontent.com/60887273/167288822-9bb1ab4d-de23-481f-a7bc-9dd4fc41b6e2.mov

https://user-images.githubusercontent.com/60887273/167288831-f7dbbb64-b75d-4245-a61c-6141a718eb23.mov
### How it was captured
If you are completly new to the Mining workflow, check out [AnimeCards.site](https://animecards.site/) before jumping into my specific workflow.

The main workhorse of everything is [Game2Text](https://game2text.com/), though the setup in the case of Genshin is not straight forward. Game2Text is a locally run server, that opens as localhost in your browser. Game2Text then allows you to combine a couple of things: Capture a window's content via FireFox's and Chrome's native window capture feature, run a region of the window through OCR, like the offline and opensource [tesseract](https://github.com/tesseract-ocr/tesseract) or the more powerful online service [ocr.Space](https://ocr.space/) and finally allow you to translate the text with help of popular plug-ins like [YomiChan](https://github.com/FooSoft/yomichan).

Game2Text has native Anki Connect integration, which builds Anki card from the captured screenshot, the currently selected word and a the dictionary definiton. However, this native Anki integration sometimes fails to recognize phrases, that more complex dictionary suites in YomiChan detect. Luckily Game2Text can essentially give you the best of both worlds, since you can just use directly YomiChan to create a card. Though in that case, you have to handle screenshots yourself.

Genshin fails to get captured by the browser, unless it is in window mode. There is no proper borderless window mode in Genshin unfortunately, unless you use a patcher to get a borderless window. Another option is to run OBS in administrator mode, capture the game, open up a full-screen "source monitor"-window of Genshin's source signal and let the browser capture that. This is the option I went with on my Desktop PC. On my Laptop with a dedicated Nvidia GPU this leads to massive performance loss however, presumably because of saturating memory bandwidth due to some weird interaction between the iGPU and the main GPU causing a memory bottleneck somehow.

If Game2Text has a hook-script for the game in question, then it can hook into the game's memory and read out the dialog strings, forgoing the sometimes imprecise OCR. No such hook-script exists for Genshin Impact (to my knowledge). I tried to create one by poking around with CheatEngine, but there was no obvious strings-block in memory. I decided to drop that approach due to the worry of having my account banned. If OCR is imprecise, there is always the onscreen dialog box to check against.
#### Handling Audio
Across multiple systems, Game2Text fails to create a card with sound for me though. It successfully captures sound in .wav files, but transcodes them to fully silent .mp3 files, which it attaches to the cards. So instead of working with the .wav files, I simply let Audacity capture the sound output via its "Windows WASAPI" mode and the thus unlocked speaker loopback record method in a non-stop recording. On dialog I would select the needed passage and via a macro bound to a hot-key, perform the conversion to mono, normalization and export to an mp3 file.

Originally, I set all audio to be normalized based on setting the peak sample to -3db via Audacity. This turned out to be not quite optimal, as the amount of voice profiles is very broad. With peak sample normalization bright and dark voices did not end up playing back at the same loudness, since it does not account for human hearing being more sensitive to some frequencies compared others. I batch-reencoded every audio file to be normalized to -15LUFS loudness instead, the more modern approach. Although the difference was subtle, the dialogue sounded a bit more balanced from card to card after that.

It would be optimal to have no music mixed in with the dialog for the sake of cleaner dialog sound during card reviews. However, the music is so incredibly good, that it would have not been even half as enjoyable to go through the game without the music. So often background music plays with the cards. Worst offender being Liyue Harbor's background track, which manages to drown out dialog [during its crescendo](https://youtu.be/t1O7LpOTBfM?t=318).
#### The actual workflow
When I did not recognize a word, I would tab out of the game, select the rectangle in Game2Text to get the transcript. I would then use YomiChan to aid me in understanding the sentence. When the "logs" screen of Game2Text properly recognized the phrase in question, I would then use it to create the card. When not, I would use YomiChan and manually post the screenshot. Then I would tab into audacity, select the needed passage, press the hotkey to export the sound as an mp3 into a folder I had open and Drag&Drop to the current Anki card.

Sometimes there was no dictionary sound reading in the Game2Text log screen, but in YomiChan - or vice-versa. In that case I would play the dictionary sound from the other source, let Audacity capture it and again export that sound passage. (Technically you can get the sound file directly or by creating a duplicate card, but that was too much of a hassle) Finally, if none of the two sources had a dictionary reading, I would manually check the [JapanesePod101 dictionary](https://www.japanesepod101.com/japanese-dictionary/), which surprisingly has a ton of obscure vocab readings as sound files. Just make sure to un-tick 'most common 20.000 words', tick 'Include vulgar words' and switch the mode from 'ls' to 'Starts with' to find more complex phrases.

This concludes my little write-up about the Genshin Impact part of my Anki mining deck.
## Jōyō kanji Unicodes lists
These files allow you to select or subset just the  2136 [Jōyō kanji](https://en.wikipedia.org/wiki/J%C5%8Dy%C5%8D_kanji) Unicode codepoints for Japanese. It's a good middle ground between having only Kana and including the all 21000 characters of the complete [CJK block](https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)) for embedded use-cases. How to use these files and how to subset fonts is [covered here](https://github.com/Immediate-Mode-UI/Nuklear/wiki/Complete-font-guide#subsetting-compressing-appending-to-the-binary). Note, that not all HEX codes are 4 digit, the last one 𠮟 is HEX 20B9F. Eyes open, in case your program you use these HEX values in is picky about formatting. It's a late edition to the Unicode standard [because reasons (article “𠮟る” と “叱る” by @hydrocul)](https://hydrocul.github.io/wiki/blog/2014/1201-shikaru.html) and is thus the only 5 digit HEX Jōyō kanji. The variant 叱 53F1 is also listed, since it's usually the assumed default. ([Archive link in case that article goes down](https://web.archive.org/web/20210405065949/https://hydrocul.github.io/wiki/blog/2014/1201-shikaru.html)).
* [Nuklear](https://github.com/Immediate-Mode-UI/Nuklear) Codepoints: [joyo-kanji-unicode-nuklear.txt](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/joyo-kanji-unicode-nuklear.txt)
* [PyFTsubset](https://fonttools.readthedocs.io/en/latest/subset/index.html) Codepoints: [joyo-kanji-unicode-pyftsubset.txt](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/joyo-kanji-unicode-pyftsubset.txt)
* Raw-Hex Codepoints: [joyo-kanji-unicode-unformatted-hex.txt](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/joyo-kanji-unicode-unformatted-hex.txt)
### Regional variants
Finally, the usual applies if your are working with CJK fonts: Double, triple and quadruple check the font's intended region or in rare cases, how the font file organizes those regions internally. Eg. Google's Noto [offers their CJK fonts](https://github.com/googlefonts/noto-cjk) in 'CJK SC (Simplified Chinese)', 'CJK TC (Traditional Chinese)', 'CJK JP (Japanese)', etc. Thanks to Unicode's aweful decision of [Han unification](https://en.wikipedia.org/wiki/Han_unification), regional Sinograph variants are mapped to the same Unicode code point.

When my smartphone didn't have Japanese set as an optional language, the OS assumed the chinese variants by default and I was suddenly confused as to why I could not read some "Japanese" words in some apps. Such presumed defaults create issues in many [other](https://community.wanikani.com/t/userscript-anime-context-sentences/54003/83?u=frostkiwi) circumstances as well. Here is the difference with my Anki decks.
![](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/readme-img/anki-regional-example.png)
# Jo_MPEG converted to C
jo_mpeg is a C++ [single header library](https://github.com/nothings/single_file_libs) written by [Jon Olick](https://www.jonolick.com/home/mpeg-video-writer), which creates MPEG videos (without audio). It is [listed as a C++ only library](https://github.com/nothings/single_file_libs#video) in stb's single header library collection. However, only the & reference format is what makes this library C++ only. Replacing those with simple pointers makes this compile with both C and C++: [jo_mpeg.h](https://github.com/FrostKiwi/treasurechest/blob/main/jo_mpeg.h)

( I also found this: https://github.com/yui0/slibs/blob/master/jo_mpeg.h, but it is one version behind. )
# Just-a-textbox
![](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/readme-img/hello-textbox.png)

As the name suggests, it's [just a minimal HTML](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/just-a-textbox.html) file serving up a single textbox with a large font size.
* Why would this be useful?
  * Several screen readers and translation tools like [YomiChan](https://github.com/FooSoft/yomichan) are browser based. As such it's convinient to copy paste something into a browser to interface with those readers and plugins. In fact, software like [Game2Text](https://game2text.com/) run a local webserver to interface with your default web browser for this very reason.
* Why not just use any textbox of any online translator like deepl.com? 
  * I constantly use YomiChan to help me read Japanese content, including client's E-Mails with confidential Company information. Pasting client information to an online translator, which phones home to offshore servers for their translations brakes privacy laws on so many levels, it's not even funny. All that just to get text into a textbox. Thus this simple textbox html ensures everything stays offline. There are of course bigger software packages that solve this OS wide, but I'm very much used to YomiChan now and a single TextBox is all I need.

![](https://raw.githubusercontent.com/FrostKiwi/treasurechest/main/readme-img/Textbox%2BYomichan.png)
## WaniKani Japanese prompts - userscript
This userscript is a modification of the script ["WK Custom Review Question (KunOn+)"](https://greasyfork.org/en/scripts/8193-wk-custom-review-question-kunon) created by Greasyfork user [hoovard](https://greasyfork.org/en/users/9284-hoovard) for the Kanji learning service [WaniKani](https://www.wanikani.com/). It modifies the script to display a Japanese translation of the prompts on review questions. The original script failed to do so and showed only half of the Japanese prompt, failing to concatenate the needed strings. This fix was discussed in [this WaniKani forum thread](https://community.wanikani.com/t/the-new-and-improved-list-of-api-and-third-party-apps/7694/553?u=frostkiwi). Either something about WaniKani changed since the script was released in 2015 or the script was never finished. Now the full translations is shown, with の added where needed. Get it [on Greasyfork](https://greasyfork.org/en/scripts/444836-wanikani-japanese-review-questions) or directly from this [Github repo](https://github.com/FrostKiwi/treasurechest/raw/main/WaniKani%20Japanese%20Review%20Questions.user.js).

![wanikani](https://user-images.githubusercontent.com/60887273/167878545-e96899cb-f225-4848-9c32-9e41bf804ada.png)
